from abc import ABC, abstractmethod
from typing import Dict, Any
import json
import requests

# Type alias for configuration
Config = Dict[str, Any]

class Assistant(ABC):
    def __init__(self):
        self.api_key = None
        self.base_url = None
    
    def initialize(self, cfg: Config):
        try: 
            self.api_key = cfg["AGENT_API_KEY"]
            self.base_url = cfg.get("AGENT_URL", "https://api.openai.com/v1")
        except KeyError as e:
            raise e

    @abstractmethod
    def query(self, prompt: str) -> str:
        pass

    @staticmethod
    def build_assistant(cfg: Config) -> 'Assistant':
        # For now, return a default LLM assistant
        # You can extend this to support different assistant types
        assistant = LLMAssistant()
        assistant.initialize(cfg)
        return assistant


class LLMAssistant(Assistant):
    def query(self, prompt: str) -> str:
        """Send a query to the LLM API using requests"""
        if not self.api_key or not self.base_url:
            raise ValueError("Assistant not properly initialized")
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-3.5-turbo",  # Default model, can be configurable
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 1000,
            "temperature": 0.7
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            return result["choices"][0]["message"]["content"]
            
        except requests.exceptions.RequestException as e:
            raise Exception(f"API request failed: {e}")
        except (KeyError, IndexError) as e:
            raise Exception(f"Invalid API response format: {e}")


class ReportAnalyzer: 
    def __init__(self, cfg_path: str):
        # we will use an LLM model to evaluate the report
        self.cfg_path = cfg_path

    def _load_config(self):
        """Load configuration from file or environment"""
        try:
            with open(self.cfg_path, 'r') as f:
                self.cfg: Config = json.load(f)
        except FileNotFoundError:
            # Fallback to environment variables or default values
            import os
            self.cfg: Config = {
                "AGENT_API_KEY": os.getenv("AGENT_API_KEY", "your-api-key-here"),
                "AGENT_URL": os.getenv("AGENT_URL", "https://api.openai.com/v1")
            }

    def _load_agent(self):
        """Initialize the LLM agent"""
        if not hasattr(self, "cfg"):
            self._load_config()
        
        self.agent = Assistant.build_assistant(self.cfg)

    def _build_prompt(self, report: str, report_type: str = "DAILY") -> str:
        tmp = "Today" if report_type == "DAILY" else "This week"
        return f"""
        Please analyze the following {report_type.lower()} calendar and time tracking summary. Your job is to simulate a fast, focused evening performance review that helps the user understand:

        1. Whether they followed their plan
        2. If they were overworking, underworking, or in balance
        3. What reminders need cleanup or rescheduling
        4. General patterns and tactical recommendations

        ---

        📋 Calendar Summary:
        {report}

        ---

        🧭 Plan vs. Reality
        - Did the user follow their plan? Answer Yes / Partially / No.
        - Did they overwork, slack off, or work at a sustainable pace?

        🔍 Reminders Check
        - Highlight completed vs. overdue reminders.
        - Comment on the backlog status: Clean / Manageable / Needs triage.

        ⚖️ Judgment Summary Table
        | Aspect        | Status       | Comment                              |
        |---------------|--------------|--------------------------------------|
        | Execution     | ✅/⚠️/✖️        | e.g., “Solid & goal-aligned”         |
        | Effort        | ✅/⚠️/✖️        | e.g., “Sustainable deep work”        |
        | Planning      | ✅/⚠️/✖️        | e.g., “No structure → drift risk”    |
        | Wellbeing     | ✅/⚠️/✖️        | e.g., “No evidence of physical care” |

        🧠 One-Line Debrief (optional)
        - "{tmp} felt ______ because ______."

        🎯 Tactical Recommendations
        - Provide 1–3 practical tips for improving scheduling, workload balance, or clarity tomorrow.
        """


    def generate_analysis(self, report: str, report_type: str = "DAILY") -> str:
        """Generate analysis of the calendar report using LLM"""
        if not hasattr(self, "cfg"):
            self._load_config()
        if not hasattr(self, "agent"):
            self._load_agent()

        prompt = self._build_prompt(report, report_type)
        
        try:
            analysis = self.agent.query(prompt)
            return analysis
        except Exception as e:
            return f"Error generating analysis: {str(e)}"


def debug():

    # Initialize components
    analyzer = ReportAnalyzer("config.json")  # Added missing config path

    try:
        # Generate calendar summary
        summary = """
        🗓️ Comprehensive Daily Summary for 2025-07-30

        📅 Planned Calendar Events:
        (none)

        ⏱️ Actual Events (Toggl):
        13:57:00–15:09:00 | MIT Manipulation Course | Growth | Note: Imported from Toggl - ID: cb76b354...
        09:55:00–11:57:00 | LeetCode | Growth | Note: Imported from Toggl - ID: ecf45304...

        🚨 Overdue Reminders:
        🔴 HKU Position | Personal | Due: Wednesday, July 30, 2025 00:00:00

        ✅ Completed Reminders:
        ✓ Leetcode | Growth
        ✓ MIT Manipulation Course | Growth

        📊 Summary:
        Calendar Events: 0 planned, 3 actual (Toggl)
        Reminders: 6 total (3 overdue, 3 completed)
        """

        # Analyze the summary
        result = analyzer.generate_analysis(summary)  # Fixed: pass summary as argument
        
        print(result)
        
    except Exception as e:
        print(f"Error: {e}")


if __name__ == "__main__":
    debug()